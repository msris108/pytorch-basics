{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LINEAR REGRESSION IN PYTORCH ####\n",
    "\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# Convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4503,  0.2345, -1.9986],\n",
      "        [-0.5834, -1.7458, -1.6562]], requires_grad=True)\n",
      "tensor([-1.4516, -1.2366], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Generate random wieghts and biases\n",
    "# Every Tensor has a flag: requires_grad\n",
    "# that allows for fine grained exclusion of subgraphs from gradient computation and can increase efficiency.\n",
    "\n",
    "# This is especially useful when you want to freeze part of your model,\n",
    "# or you know in advance that you’re not going to use gradients w.r.t. some parameters.\n",
    "# For example if you want to finetune a pretrained CNN, it’s enough to switch the requires_grad flags in the frozen base,\n",
    "# and no intermediate buffers will be saved, until the computation gets to the last layer,\n",
    "# where the affine transform will use weights that require gradient, and the output of the network will also require them.\n",
    "\n",
    "w = torch.randn(2, 3, requires_grad = True)\n",
    "b = torch.randn(2, requires_grad = True)\n",
    "\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_lr_model(x):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  34.1931, -232.0047],\n",
      "        [  23.2525, -313.9467],\n",
      "        [  40.2300, -381.9806],\n",
      "        [  82.6160, -197.0864],\n",
      "        [ -18.7702, -325.0164]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = simple_lr_model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOSS FUNTION ###\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel() # '*' for element wise multiplication, .numel -> # elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mse(targets, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x\n",
    "# In pseudo-code:\n",
    "# x.grad += dloss/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() # gradient descent -> requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4503,  0.2345, -1.9986],\n",
      "        [-0.5834, -1.7458, -1.6562]], requires_grad=True)\n",
      "tensor([[ -3183.8455,  -5236.2939,  -3096.6636],\n",
      "        [-31824.7461, -35689.6602, -21830.8008]])\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(w.grad)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset values\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w -= w.grad * 10\n",
    "    b -= b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.4503,  0.2345, -1.9986],\n",
       "         [-0.5834, -1.7458, -1.6562]], requires_grad=True),\n",
       " tensor([-1.4516, -1.2366], requires_grad=True))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  52.0177,  -91.0015],\n",
      "        [  46.8930, -128.5029],\n",
      "        [  69.2727, -161.8717],\n",
      "        [  98.9082,  -57.8891],\n",
      "        [   4.7772, -146.5318]], grad_fn=<AddBackward0>)\n",
      "tensor(26426.8027, grad_fn=<DivBackward0>)\n",
      "tensor([[ -1330.6926,  -3210.8064,  -1853.6537],\n",
      "        [-17258.4180, -20013.4902, -12162.3867]])\n",
      "tensor([ -21.8263, -209.1594])\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = simple_lr_model(inputs)\n",
    "print(preds)\n",
    "\n",
    "# Calculate the loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "\n",
    "# Adjust weights & reset gradients\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5382,  0.4008, -1.9012],\n",
      "        [ 0.3793, -0.6522, -0.9890]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EPOCHS ###\n",
    "# learning rate => 1e-5\n",
    "for i in range(2000):\n",
    "    preds = simple_lr_model(inputs)\n",
    "    loss = mse(targets, preds)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6197, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss from 40000 -> 0.6197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.9609,  70.1249],\n",
       "        [ 82.0368, 100.6308],\n",
       "        [119.4038, 133.3403],\n",
       "        [ 21.3001,  37.1386],\n",
       "        [101.2111, 118.7570]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USING BUILT IN FUNCTIONS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DataLoader(train_ds, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x, y in train_ds:\n",
    "#     print(x)\n",
    "#     print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(3, 2)\n",
    "# model.weight\n",
    "# model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.mse_loss(model(inputs), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 24.8331\n",
      "Epoch [20/1000], Loss: 14.9055\n",
      "Epoch [30/1000], Loss: 8.8656\n",
      "Epoch [40/1000], Loss: 10.5480\n",
      "Epoch [50/1000], Loss: 4.2775\n",
      "Epoch [60/1000], Loss: 2.0864\n",
      "Epoch [70/1000], Loss: 3.2895\n",
      "Epoch [80/1000], Loss: 4.5299\n",
      "Epoch [90/1000], Loss: 1.7754\n",
      "Epoch [100/1000], Loss: 3.4382\n",
      "Epoch [110/1000], Loss: 4.6748\n",
      "Epoch [120/1000], Loss: 1.1677\n",
      "Epoch [130/1000], Loss: 2.2987\n",
      "Epoch [140/1000], Loss: 2.6281\n",
      "Epoch [150/1000], Loss: 3.8099\n",
      "Epoch [160/1000], Loss: 3.4619\n",
      "Epoch [170/1000], Loss: 3.1256\n",
      "Epoch [180/1000], Loss: 0.6880\n",
      "Epoch [190/1000], Loss: 2.0739\n",
      "Epoch [200/1000], Loss: 1.8151\n",
      "Epoch [210/1000], Loss: 3.8886\n",
      "Epoch [220/1000], Loss: 2.0101\n",
      "Epoch [230/1000], Loss: 2.4537\n",
      "Epoch [240/1000], Loss: 1.7163\n",
      "Epoch [250/1000], Loss: 1.3765\n",
      "Epoch [260/1000], Loss: 1.5438\n",
      "Epoch [270/1000], Loss: 1.4313\n",
      "Epoch [280/1000], Loss: 1.3403\n",
      "Epoch [290/1000], Loss: 0.5866\n",
      "Epoch [300/1000], Loss: 0.7656\n",
      "Epoch [310/1000], Loss: 1.4052\n",
      "Epoch [320/1000], Loss: 0.9619\n",
      "Epoch [330/1000], Loss: 1.7103\n",
      "Epoch [340/1000], Loss: 0.8026\n",
      "Epoch [350/1000], Loss: 1.0732\n",
      "Epoch [360/1000], Loss: 0.7436\n",
      "Epoch [370/1000], Loss: 0.6723\n",
      "Epoch [380/1000], Loss: 1.4766\n",
      "Epoch [390/1000], Loss: 1.4447\n",
      "Epoch [400/1000], Loss: 0.6537\n",
      "Epoch [410/1000], Loss: 0.9152\n",
      "Epoch [420/1000], Loss: 0.5953\n",
      "Epoch [430/1000], Loss: 1.2156\n",
      "Epoch [440/1000], Loss: 1.2565\n",
      "Epoch [450/1000], Loss: 1.6201\n",
      "Epoch [460/1000], Loss: 0.7721\n",
      "Epoch [470/1000], Loss: 1.1248\n",
      "Epoch [480/1000], Loss: 1.0596\n",
      "Epoch [490/1000], Loss: 0.7364\n",
      "Epoch [500/1000], Loss: 1.0908\n",
      "Epoch [510/1000], Loss: 0.5638\n",
      "Epoch [520/1000], Loss: 0.7961\n",
      "Epoch [530/1000], Loss: 0.7151\n",
      "Epoch [540/1000], Loss: 0.5631\n",
      "Epoch [550/1000], Loss: 0.6922\n",
      "Epoch [560/1000], Loss: 0.7678\n",
      "Epoch [570/1000], Loss: 0.5988\n",
      "Epoch [580/1000], Loss: 0.9743\n",
      "Epoch [590/1000], Loss: 0.8296\n",
      "Epoch [600/1000], Loss: 0.6946\n",
      "Epoch [610/1000], Loss: 0.8103\n",
      "Epoch [620/1000], Loss: 0.6171\n",
      "Epoch [630/1000], Loss: 0.6053\n",
      "Epoch [640/1000], Loss: 0.8573\n",
      "Epoch [650/1000], Loss: 0.7506\n",
      "Epoch [660/1000], Loss: 0.6758\n",
      "Epoch [670/1000], Loss: 0.9237\n",
      "Epoch [680/1000], Loss: 0.6114\n",
      "Epoch [690/1000], Loss: 0.6576\n",
      "Epoch [700/1000], Loss: 0.5504\n",
      "Epoch [710/1000], Loss: 0.5453\n",
      "Epoch [720/1000], Loss: 0.8584\n",
      "Epoch [730/1000], Loss: 0.5909\n",
      "Epoch [740/1000], Loss: 0.7397\n",
      "Epoch [750/1000], Loss: 0.6111\n",
      "Epoch [760/1000], Loss: 0.5225\n",
      "Epoch [770/1000], Loss: 0.7754\n",
      "Epoch [780/1000], Loss: 0.3990\n",
      "Epoch [790/1000], Loss: 0.5724\n",
      "Epoch [800/1000], Loss: 0.5386\n",
      "Epoch [810/1000], Loss: 0.3926\n",
      "Epoch [820/1000], Loss: 0.4301\n",
      "Epoch [830/1000], Loss: 0.4073\n",
      "Epoch [840/1000], Loss: 0.8936\n",
      "Epoch [850/1000], Loss: 0.7333\n",
      "Epoch [860/1000], Loss: 0.6141\n",
      "Epoch [870/1000], Loss: 0.5564\n",
      "Epoch [880/1000], Loss: 0.3972\n",
      "Epoch [890/1000], Loss: 0.5530\n",
      "Epoch [900/1000], Loss: 0.6670\n",
      "Epoch [910/1000], Loss: 0.7442\n",
      "Epoch [920/1000], Loss: 0.8509\n",
      "Epoch [930/1000], Loss: 0.6264\n",
      "Epoch [940/1000], Loss: 0.7534\n",
      "Epoch [950/1000], Loss: 0.5457\n",
      "Epoch [960/1000], Loss: 0.5281\n",
      "Epoch [970/1000], Loss: 0.5510\n",
      "Epoch [980/1000], Loss: 0.4901\n",
      "Epoch [990/1000], Loss: 0.7193\n",
      "Epoch [1000/1000], Loss: 0.4191\n"
     ]
    }
   ],
   "source": [
    "fit(1000, model, loss_fn, opt, train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
